---
title: 单机事务以及分布式事务
subtitle:
date: 2024-10-30T10:59:27+08:00
slug: f932588
draft: false
author:
  name:
  link:
  email:
  avatar:
description:
keywords:
license:
comment: false
weight: 0
hiddenFromHomePage: false
hiddenFromSearch: false
hiddenFromRelated: false
hiddenFromFeed: false
summary:
resources:
  - name: 隔离性.png
    src: 隔离性.png
  - name: featured-image-preview
    src: featured-image-preview.jpg
toc: true
math: false
lightgallery: false
password:
message:
repost:
  enable: false
  url:

# See details front matter: https://fixit.lruihao.cn/documentation/content-management/introduction/#front-matter
---





## 事务
在一个苛刻的数据存储环境中，会有许多可能出错的情况，例如：
- 数据库软件或硬件可能会随时失效（包括正在执行写操作的过程中）
- 应用程序可能随时崩溃（包括一系列操作执行到中间某一步）
- 应用于数据库节点之间的链接可能随时中断，数据库节点之间也存在同样的问题
- 多个客户端可能同时写入数据库，导致数据覆盖
- 客户端可能读到一些无意义的，部分更新的数据
- 客户端之间由于边界条件竞争所引入的各种奇怪的问题

数据库的事务功能一直是简化这些问题的首选机制
### ACID
#### 原子性（Atomicity）
多个写操作，要么全部执行成功，要么全部不执行。只要其中一个指令执行失败，所有的指令都执行失败，数据进行回滚，回到执行指令前的数据状态。
#### 一致性（Consistency）
ACID中的一致性主要是指对数据有特定的预期状态，任何数据更改必须满足这些状态约束（或者恒等条件），事务的执行使数据从一个状态转换为另一个状态，但是对于整个数据的完整性保持稳定。例如对于一个账单系统，账户的贷款余额应和借款余额保持平衡。这种一致性本质上要求应用层来维护状态一致，应用程序有责任正确地定义事务来保持一致性。这不是数据库可以保证的事情：即如果提供的数据修改违背了恒等条件，数据库很难检测进而阻止该操作。
#### 隔离性（Isolation）大多数据库都支持多个客户端同时访问，如果访问相同的数据，那么就可能出现并发问题，即带来竞争条件。

![隔离](/单机事务以及分布式事务/隔离性.png)



这里是一个计数器的例子，每个客户首先读取新值，增加1后写回新值，两次递增后结果是43，而不是44。隔离性意味着并发执行的多个事务相互隔离，不能相互交叉。经典的数据库教材把隔离定义为可串行化，这意味着可以假装它是数据库上运行的唯一事务，虽然实际上它们可能同时运行，但数据库系统要确保当事务提交时，其结果与串行化执行完全相同。然而实践中，由于性能问题很少使用串行化隔离。
#### 持久性（Durability）
当事务成功提交后，即使存在硬件故障或者数据库崩溃，事务所写入的数据也不会消失。对于单节点数据库，持久性通常意味着数据已被写入非易失性存储设备。分布式数据库的持久性意味着数据成功复制到多个节点。
### BASE
一些新型的分布式数据库，往往不是提供ACID，而是BASE。BASE没有其他保证，各个的实现方式也不相同，唯一可以确定的是，它不是ACID。
- 基本可用（Basically Available）：系统能够基本运行、一直提供服务。
- 软状态（Soft-state）：系统不要求一直保持强一致状态。
- 最终一致性（Eventual consistency）：系统需要在某一时刻后达到一致性要求。
### redo undo
mysql数据库事务的原子性，一致性和持久性需要通过redo和undo log来解决。

![redo](/单机事务以及分布式事务/redo.png)

- 第一步：先将原始数据从磁盘中读入内存中来，修改数据的内存拷贝
- 第二步：生成一条重做日志并写入redo log buffer，记录的是数据被修改后的值
- 第三步：当事务commit时，将redo log buffer中的内容刷新到 redo log file，对 redo log file采用追加写的方式
- 第四步：定期将内存中修改的数据刷新到磁盘中

redo log可以看做wal，每次是追加写。因为修改磁盘中b+树结构的数据，是随机写，性能不好，因此在事务提交时先写入redo log，然后定时同步内存数据修改磁盘，如果系统崩溃，那么可以从redo log中从头开始进行恢复，保证了持久性。

undo log主要记录的是数据的逻辑变化，为了在发生错误时回滚之前的操作，需要将之前的操作都记录下来，然后在发生错误时才可以回滚。undo日志，只将数据库逻辑地恢复到原来的样子，在回滚的时候，它实际上是做的相反的工作，比如一条INSERT ，对应一条 DELETE，对于每个UPDATE,对应一条相反的 UPDATE,将修改前的行放回去。undo日志用于事务的回滚操作进而保障了事务的原子性。由于数据库的steal策略，事务在更新一个页面后，数据库是可以把未提交的数据刷入磁盘的，这时如果发生崩溃，恢复时数据库就需要从undo中找到数据库的更新前像。
```
假设有A、B两个数据，值分别为1,2.2
1. 事务开始3
2. 记录A=1到 undo log4
3. 修改A=35
4. 记录A=3到 redo log6
5. 记录B=2到 undo log7
6. 修改B=48
7. 记录B=4到 redo log9
8. 将redo log写入磁盘10
9. 事务提交
```

### 弱隔离级别
数据库一直视图通过事物隔离来对应用开发者隐藏内部的各种并发问题。从理论上说，隔离性是假装没有发生并发，可串行化隔离意味着数据库保证了事务的最终执行结果与串行执行结果相同。许多数据库并不愿意牺牲性能，因而更多倾向于采用较弱的隔离级别，它可以防止某些但并非全部的并发问题，这些弱隔离级别理解起来更为困难，甚至可能会带来一些难以捉摸的隐患，但在实践中还是被广泛使用。

SQL-92 标准定义了 4 种隔离级别：读未提交 (READ UNCOMMITTED)、读已提交 (READ COMMITTED)、可重复读 (REPEATABLE READ)、串行化 (SERIALIZABLE)

| **Isolation Level** | **Dirty Write** | **Dirty Read** | **Fuzzy Read** | **Phantom**  |
| ------------------- | --------------- | -------------- | -------------- | ------------ |
| READ UNCOMMITTED    | Not Possible    | Possible       | Possible       | Possible     |
| READ COMMITTED      | Not Possible    | Not possible   | Possible       | Possible     |
| REPEATABLE READ     | Not Possible    | Not possible   | Not possible   | Possible     |
| SERIALIZABLE        | Not Possible    | Not possible   | Not possible   | Not possible |

### 读已提交
读已提交是最基本的事务隔离几倍，它只提供以下两个保证：
1. 读数据库时，只能看到已成功提交的数据（防止脏读）
2. 写数据库时，只会覆盖已成功提交的数据（防止脏写）
### 防止脏读
假定某个事物已经完成部分数据写入，但事务尚未提交，此时另一个事务可以看到尚未提交的数据，那就是脏读

![脏读](/单机事务以及分布式事务/脏读.png)

没有脏读时，用户2只有在用户1的事务提交之后才能看到x的新值。

脏读有两种情况，一种如图所示，用户2第二次读取将返回x=3，如果此时用户1的事务发生回滚，那么用户2的数据就是错误的。

还有一种情况是在一个事务中写多个对象，例如转账，第一个账户扣除完成，第二个账户还未增加时，此时第二个事务来读取时就会发现账户总金额变少了。
### 防止脏写
如果两个事物同时尝试更新相同的对象，如果第一个写入尚未提交，就被第二个事务的写入覆盖，那就是脏写。读已提交级别下所提交的事务可以防止脏写，通常的方式是推迟第二个写请求，直到前面的事务完成提交（或者中止）。

防止脏写可以避免以下的并发问题：
- 如果事物需要更新多个对象。

  ![脏写](/单机事务以及分布式事务/脏写.png)

如果所示，购买车辆时需要同时修改车辆和发票数据，但最终的结果是车辆和发票不是同一个用户。
- 但是，读已提交隔离并不能解决计数器的竞争问题。计数器中，第二次写入确实在第一个事务提交后才执行，虽然不属于脏写，但结果仍然是错误的

#### 实现读已提交
数据库通常采用行级锁来防止脏写：当事务想修改某个对象时，必须首先获取该对象的锁，然后持有锁知道事务提交或中止，这保证同一时刻只会有一个事务获取该锁。其他事务如果尝试获取该锁，那么就必须等待，这里会有死锁检测的问题。

那么如何防止脏读，一种选择是使用相同的锁，读取某个对象时也需要获取它的锁。但是这会导致读事务延迟增加，因此很多数据库会同时保存两个版本，一个是旧值，另一个是未提交的版本，在写事务提交前，其他事务都读取旧值

### 快照隔离与可重复读

![](/单机事务以及分布式事务/不可重复读.png)

**读倾斜（不可重复读）**：两个账户中各有1000美元，alice读取时首先读取账户1为500，读取另一个账户时，中间发生了一次转账，那么最终结果她只有900

快照隔离可以解决如上问题，即每个事物都从数据库的一致性快照读取

**实现快照隔离**
快照隔离同样采用写锁防止脏写

考虑到多个正在进行的事务可能会在不同的时间点查看数据库状态，所以数据库保存了多个不同的提交版本，这种技术被称为MVCC。当事务开始时，首先赋予一个唯一的、单调递增的事务id，每当数据被写入时，都会带上这个事务id，而在读取时通过比较数据上的事务id和当前事务的id的大小，判断是否可见。如果是lsm-tree，那么天然就是MVCC的。
![](/单机事务以及分布式事务/快照隔离.png)

**一致性快照的可见性规则**
1. 事务开始时，数据库列出所有正在进行中的事务，这些事务当前所做的部分修改不可见
2. 所有中止的事务不可见
3. 事务id大于当前事务id的修改不可见
4. 除此之外，其他写入可见

**索引和快照隔离**

一种方案是索引直接指向对象的所有版本，然后再进行过滤，只有当历史版本删除时才去删除索引

追加/写时复制的B-Tree，当需要更新时，总是创建一个新的副本，并拷贝必要的内容，然后让父节点，或者递归向上直到树的root节点都指向新的节点，那些不受影响的页面不需要复制，保持不变并被父节点所指向。这种采用追加的B-Tree，每个写入事务都会创建一个新的B-Tree root，代表该时刻的一致性快照，有利于查询性能。
![](/单机事务以及分布式事务/追加写时复制.png)

**可重复读与命名混淆**
快照隔离具体到不同的数据库，Oracle称之为可串行化，PG和Mysql称之为可重复读。DB2的可重复读实则是可串行化级别隔离。

这种命名混淆的原因是sql标准没有定义快照级别隔离。且sql标准对隔离级别的定义还是存在一些缺陷，且不能做到与实现无关。尽管很多数据库都实现了可重复读，但提供的保证却有所不同。

#### 更新丢失
更新丢失可能发生在这样的场景中：应用程序从数据库读取某些值，基于读取的值做出修改，然后写回新值（read-modify-write）。当有两个事物同时操作一个对象时，第二个写操作就有可能覆盖第一个写操作。
1. 递增计数器，或更新账户余额
2. 修改部分内容，例如文档数据库，修改json的某个元素
3. 同时编辑wiki界面

**原子写**

例如在关系数据库中进行 update table1 set value = value+1 。

RocksDB提供了merge操作。

MongoDB提供了对json文档部分修改的原子操作。

原子操作通常通过对象的独占锁来实现。这里介绍一下RocksDB的merge，我们知道RocksDB的数据结构是lsm-tree，put操作可以简单认为是追加到文件最后面，get时文件从后往前读，读到最新的put即可。merge操作也是如此，从后往前读到最新的put，在此过程中的merge会被记录下来，然后追加到put的值上，下面的例子中，get时读取到一开始a=5，经历了两次+1，所以最终结果是7，compact时会将merge操作进行合并，所以对get的影响也可控。


文件头
put a = 5
merge a +1
merge a +1

文件头
put a = 7


**显式加锁**

如果应用程序不支持内置的原子操作，另一种防止更新丢失的方法是应用程序显示加锁，然后执行读-修改-写，此时其他事务如果要修改相同对象，那么就会等待该锁。有些数据库支持getForUpdate操作，该操作会对返回的行进行加锁。
```sql
begin transaciton;
select * from table1 where id = 1 for update;
// do check
update table1 set ...;
commit;
```
RocksDB支持乐观事务和悲观事务，悲观事务在getForUpdate时锁定该key，如果已经被占用会抛出异常，乐观事务在getForUpdate时记录该key当前的版本id，在commit时判断当前的id是否和记录的一致，如果不一致说明已经被修改，那么就会抛出异常。

**自动检测更新丢失**
原子操作和锁都是通过强制读-修改-写操作序列串行化执行来防止更新丢失，另一种思路是先让他们并发执行，但如果事务管理区检测到了更新丢失的风险，则会中止当前事务，并强制回退到读-修改-写方式。该方法的一个优点是数据库完全可以借助快照隔离来高效的执行检查，PG的可重复读，Oracle的可串行化，sql server的快照隔离，都可以自动的检测何时发生了更新丢失，但是mysql的Innodb却不支持。

**原子比较和设置**
使用该操作可以避免更新丢失，即只有数据没有发生变化时才允许更新，例如以下sql。`
update table1 set version=1001 where id =1 and version=1000`有些kv也会提供类似的原子api：put(key, oldValue, newValue)

**冲突解决和复制**
对于支持多副本的数据库，防止更新丢失还需要考虑另一个维度：不同节点并发修改数据，在后续的同步过程中会出现冲突，如果冲突的结果方法是最后写入获胜LWW，那么就容易出现更新丢失。

#### 写倾斜和幻读
假设有一个医生值班系统，允许医生请假，但需要保证最起码有一个医生值班。此时有两个医生同时发起请假请求，如下图。

![](/单机事务以及分布式事务/写倾斜.png)

最终结果就是两个医生都请假成功，导致没有人值班。这就是写倾斜，它既不是脏写，也不是更新丢失，因为两个事物修改的是不同的两个对象，单对象的原子操作不起作用。基于快照隔离也不支持检测写倾斜，防止写倾斜要求真正的可串行化隔离。

可以将写倾斜定义成更广义的更新丢失问题：如果两个事务读取相同的一组对象，然后更新其中的一部分，不同的事务更新不同部分，则可能发生写倾斜；而不同的事务如果更新的同一个对象，则可能发生脏写或者更新丢失。

一种次优的解决方案是getForUpdate对所有行进行加锁，但如果根本没有返回行，这里也无从加锁，譬如说这里修改成最多只能有一个医生请假。

### 串行化
串行化保证事务即便并发执行，但最终的结果与串行执行的结果相同。

**实际串行执行**
最直接的方法就是串行执行。为了避免多次的数据库io交互，可以使用存储过程进行封装。

串行执行所有事物使得并发控制更为简单，但是吞吐量被限制在单个cpu上，虽然只读事务可以在单独的快照上执行，但是对于高写入的场景，单线程事务处理很容易成为瓶颈。

如果可以找到一种方法对数据进行分区，使得事物只操作单个分区的数据，那么可以每个分区一个线程。如果存在跨分区事务，那么就需要事务的协调机制，将会导致性能下降。

**两阶段加锁 2PL**

在防止脏写中，已经讨论过通过对对象加锁的方式，即第二个写入等待前面事务完成。

两阶段加锁类似，但是强制性更高，多个事物可以同时读取同一对象，但只要出现任何写操作，则必须加锁以独占访问：
1. 如果事务A已经读取某对象，此时事务B想要写入该对象，必须等待事务A完成
2. 如果事务A已经修改某对象，还未提交，事务B想到读取该对象必须等待事务A完成

可以看作每个对象都有一把读写锁，读事务申请读锁，读锁之间可以共享，写事务申请写锁，写锁之间不共享，写锁和读锁不共享

dremio在RocksDBStore这层目前就是通过应用内增加读写锁，之间想过替换为事务来做到cas的操作，但因为有大value的优化，发现不可行。大value时会写入本地文件，put或delete更新时会将该文件删除，即数据并不是MVCC的，老版本的文件被删除后就不是可读的了，所以读写之间需要有隔离。

**两阶段加锁的性能**

两阶段加锁除了锁的获取和释放，但更重要的是降低了事务的并发性。一旦多个事物同时访问相同对象，或者一个事务获得了大量对象的锁且执行缓慢，那么其他事务会陷入长时间的排队等待，死锁也将非常频繁

**谓词锁**

可串行化需要解决幻读问题。两阶段加锁可以解决之前举例的两个医生同时请假的情况，但没有数据加锁的情况下还需要讨论。

例如会议室预定，如果事务在查询某个时间段内一个房间的预定情况，则另一个事务不能同时去插入或更新同一时间段内该房间的预定情况，但它应可以修改其他房间的预定情况，或者修改该房间的其他时间段预定。技术上讲，我们需要引入谓词锁，它的作用类似于共享/独占锁，区别是，它并不属于某个特定的对象，而是作用于满足某些查询条件的所有对象。这里有一张预定记录表，每次预定插入一行。

room_id start_time end_time

```sql
select * from bookings where room_id = 123 and start_time &lt; '2022-10-12 13:00:00'
and end_time &gt; '2022-10-12 12:00:00'
```
谓词锁会限制如下访问：
1. 如果事务A想要读取某些满足查询条件的对象，必须以共享某些获得查询条件的谓词锁，如果事务B正持有任何一个匹配该条件的互斥锁，那么事务A需要等待事务B
2. 如果事务A想要修改，删除，插入任何对象，则必须检查所有旧值和新值是否与现有任何的谓词锁匹配， 如果事务B正持有这样的谓词锁，那么事务A需要等待事务B

将两阶段加锁与谓词锁结合使用，数据库可以防止所有形式的写倾斜和其他竞争条件，隔离变得真正可串行化。

**索引区间锁**

但因为检查谓词锁是否匹配比较耗时，因此其性能不佳，因此大多数使用2PL的数据库实现的是索引区间锁（或者next-key locking），本质上它是对谓词锁的简化或者近似。

简化谓词锁的方式是将其保护的对象扩大化。在会议室预定中，一种方式是保护该房间的所有时间段，即该房间其他时间段也不允许预定，或者保护所有房间的指定时间段。一般而言，数据库会在room_id，或者start_time和end_time上添加索引，如果索引在room_id上，那么锁可以加在该索引上，索引在时间上同理。如果没有合适的索引，数据库回退到对表添加共享锁。

**可串行化的快照隔离**
目前为止，在数据库并发上，两阶段加锁可以保证串行化，但性能差强人意；弱隔离级别虽然性能不错，但是容易引起各种边界条件（如更新丢失，写倾斜，幻读）。

可串行化的快照隔离提供了完整的可串行保证，而性能损失相较于快照隔离损失很小。目前其在PG和FoundationDB上已有应用。

**悲观和乐观的并发控制**

两阶段加锁是一种典型的悲观并发控制，某种意义上讲，串行执行是一种极端悲观，在数据库实例上添加了锁。相比之下，可串行快照隔离是一种乐观并发控制，如果可能发生冲突，事务会继续执行而不是终止，只有在提交时会进行检查是否违反了隔离性原则。

乐观并发控制如果冲突很多，则性能不佳，大量事务会终止，如果采用重试的策略，更可能导致冲突加剧，数据库性能下降。

**检查是否读取了过期的MVCC对象**
![](/单机事务以及分布式事务/事务.png)

在医生同时请假的场景中，事务43在读取时，事务42没有提交，所以读取到的Alice状态扔是值班状态，在提交时，由事务管理器进行检查，发现该行数据已经有新的版本，那么就中止。

**检查写是否影响了之前的读**
![](/单机事务以及分布式事务/事务2.png)

ssi同样使用了索引区间锁，只有一点差异：不会阻塞其他事务

两个事务同时读取了1234这个索引，事务42修改并提交，事务43提交时发现已有事务修改索引上的数据，那么就终止

### 小结
脏读：读取其他事务尚未提交的数据，读已提交就可以防止脏读

脏写：覆盖其他事务尚未提交的数据

读倾斜（不可重复读）：多次读取，内容不同，快照隔离是最常用的防范手段，即事务总是在某个时间点上的一致性快照中读取数据，通常采用MVCC

更新丢失：两个事物同时执行读-修改-写操作，出现了后提交覆盖前提交的数据，快照隔离有些实现可以自动防止这种异常，而一些需要手动锁定查询结果

写倾斜：事务首先查询数据，根据结果修改部分数据，当事务提交时，支持决定的前提条件已不再成立，只有可串行化的隔离才能防止这种异常

幻读：事务读取了某些符合条件的对象，另一个事务执行了写入，改变了先前查询的结果。快照隔离可以防止简单的幻读，而写倾斜需要特殊的处理

如果使用弱隔离级别，那么需要应用开发人员通过显示加锁等方式手动处理这些情况。只有可串行化可以避免以上问题

串行执行：所有事务串行执行，性能差

两阶段加锁：性能有所提高，但锁冲突概率很大

可串行的快照隔离：通过乐观事务，允许多个事物并发执行，仅当事务提交时，才进行冲突检查

### Percolator
Percolator 在 Bigtable 上抽象了 5 Columns 去存储数据，其中有 3 和事务相关，另两列和通知相关，不做讨论。HBase是Bigtable的开源实现，是一个分布式kv数据库，其提供了单行事务，Percolator的目标就是给它增加跨行事务的能力。Percolator是一种优化版的2PC，但是与 常见的2PC不同，它并没有一个单独的coodinator的角色，而是作为一个库将所有逻辑放在客户端实现，只需要下层存储支持单行事务即可。原始的Percolator事务模型中，下层的存储节点可以对于上层事务完全无感知。Percolator完美体现了google分层的思想，BigTable在分布式存储上构建分布式kv，Percolator又在BigTable上构建分布式事务。为了确定事务的先后顺序，Percolator还要求一个全局的授时中心，用于获取全局有序的递增时间戳（比如TiDB中的pd组件）。

Percolator事务实现了SI隔离级别（TiDB中将它作为RR）。每个事务都从授时中心获取两个时间戳：startTS 和 commitTS，startTS 在事务开始时获取，commitTS在事务结束时获取，事务之间通过这两个时间戳来确定先后。例如有两个事务T1和T2，如果T1的commitTS小于T2的startTS，则认为T1发生在T2之前 ，如果两个事务的时间戳区间[startTS, commitTS]存在交叉，则两个事务是并发的。在SI隔离级别下一个事务只应该看到commitTS小于自己的startTS的事务所写入的数据。

![](/单机事务以及分布式事务/Percolator1.png)

**c:lock (后文中简称L列)：**
事务产生的锁，映射关系为${key,start_ts}=&gt;${lock_type,primary_key,..etc}
- key：数据的key
- start_ts：事务开始时间（分布式系统需要能提供全局唯一的时间戳，用来保证事务不冲突）
- lock_type：事务执行时，会从所有执行写操作的行中随机选一行作为primary，lock_type赋值为：primary_lock,其与的行lock_type赋值为secondary_lock。
- primary_key：当lock_type为secondary_lock时，该值指向primary行。

**c:write(后文中简称W列)**
已提交的数据信息，存储数据所对应的时间戳,映射关系为${key,commit_ts}=&gt;${start_ts}
- key :数据的key;
- commit_ts： 事务的提交时间（同样需要提供全局唯一的时间戳）
- start_ts：该事务开始时间，通过这个时间+key,可以从c:data列找到对应的数据
**c:data(后文中简称D列)**
具体存储的数据，映射关系为：${key,start_ts} -&gt; {value}
- key： 真实的key
- start_ts ： 对应事务的开始时间
- value： 真实的数据值

Percolator在写入过程中都是内存操作，然后分两个阶段提交（两阶段提交）:prewrite与commit

prewrite阶段：
1. 首先在所有行的写操作中选出一个作为 primary row，其他的为 secondary rows
2. PrewritePrimary: 对 primaryRow 写入锁以及数据，锁中记录本次事务的开始时间戳。上锁前会检查：
- 该行是否已经有别的客户端已经上锁 (Locking)
- 是否在本次事务开始时间之后，检查versions ，是否有更新 [startTs, +Inf) 的写操作已经提交 (Conflict)，已经有更大的事务提交
3. 将 primaryRow 的锁上好了以后，进行 secondaries 的 prewrite 流程：
- 类似 primaryRow 的上锁流程，只不过锁的内容为事务开始时间 startTs 及 primaryRow 的信息
- 检查的事项同 primaryRow 的一致
- 当锁成功写入后，写入 row，时间戳设置为 startTs

以上 Prewrite 流程任何一步发生错误，都会进行回滚：删除 Lock 标记 , 删除版本为 startTs 的数据。

当 Prewrite 阶段完成以后，进入 Commit 阶段

commit阶段：
1. commit primary: 写入 write CF， 添加一个新版本，时间戳为 commitTs，内容为 startTs, 表明数据的最新版本是 startTs 对应的数据
2. 删除 Lock 标记

以一个转账举例：

![](/单机事务以及分布式事务/Percolator2.png)
最初Bob有10元，Joe有2元，假设Bob要转账7元给Joe。首先需要随机选择一行最为primaryRow ，这里选择Bob。以事务开始时间戳为版本号，写入Lock与数据

![](/单机事务以及分布式事务/Percolator3.png)
随后进行secondary rows的上锁，secondary rows的锁指向primaryRow的锁

![](/单机事务以及分布式事务/Percolator4.png)

此时prewrite结束。在此过程中如果事务8进来，尝试获取锁，会发现已经被其他事务锁定，那么会失败；或者当前事务id是4，发现已经有更新的事务提交，那么获取锁失败。prewrite成功后，继续进行commit流程，首先commit primary row，客户端通过Bigtable的单行事务，清除primary行的锁，并且以提交时间戳在write写入提交标志。primary row的Write CF的写入是整个事务提交的标志，这个操作的完成就意味着事务已经完成提交了。write中写入的数据指向Bob真正存放余额的地方。完成这一步就可以向客户端返回事务commit成功了。如果在commit阶段发现primary锁已经不存在（可能因为超时被其他事务清除），则提交失败，事务回滚。
![](/单机事务以及分布式事务/Percolator5.png)

接下可以异步释放secondary rows的锁。实际上，即使在执行这一步前，客户端挂了而没能处理这些行的锁也没有问题。当其他事务读取到这样的行的数据的时候，通过锁可以找出primary行，从而判断出事务的状态，如果已经提交，则可以清除锁写入提交标志。
![](/单机事务以及分布式事务/Percolator6.png)


读隔离：
1. 检查该行是否有 Lock 标记，如果有，表示目前有其他事务正占用此行，如果这个锁已经超时则尝试清除，否则等待超时或者其他事务主动解锁。
2. 读取至 startTs 时该行最新的数据，找到最近的时间戳小于startTS的write CF，从其中读取版本号t，读取为于 t 版本的数据内容。

https://book.tidb.io/session1/chapter6/tidb-transaction-mode.html
https://storage.googleapis.com/pub-tools-public-publication-data/pdf/36726.pdf
https://www.cnblogs.com/FateTHarlaown/p/12084983.html
https://www.jianshu.com/p/bca8a678fafc
https://zhuanlan.zhihu.com/p/307438297https://blog.csdn.net/maxlovezyy/article/details/99707690

Todo
大纲或者脑图
两阶段加锁
procolate
hlc
ssi
两阶段提交
